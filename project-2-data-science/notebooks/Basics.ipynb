{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2e8b58-b0e6-4b24-a8f4-aa4c63cb4926",
   "metadata": {},
   "source": [
    "## Basic Spark Practise "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d534b-2a5d-4f2f-9d21-ebc8b2922a80",
   "metadata": {},
   "source": [
    "### Test Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f14ef7-a1a6-4d3d-afe4-d61939dda545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count Results:\n",
      "world: 1\n",
      "Welcome: 1\n",
      "to: 1\n",
      "PySpark: 2\n",
      "fun: 1\n",
      "again: 1\n",
      "Hello: 2\n",
      "is: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BasicPySparkExample\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.port\", \"6066\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\"Hello world\", \"Welcome to PySpark\", \"PySpark is fun\", \"Hello again\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "words = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"Word Count Results:\")\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c8636b-0d2c-43a6-85a4-a49d4771a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Learning\\DE Projects\\project-2-apache-spark\\notebooks\n",
      "12909234\n",
      "TradDt                  object\n",
      "BizDt                   object\n",
      "Sgmt                    object\n",
      "Src                     object\n",
      "FinInstrmTp             object\n",
      "FinInstrmId              int64\n",
      "ISIN                    object\n",
      "TckrSymb                object\n",
      "SctySrs                 object\n",
      "XpryDt                 float64\n",
      "FininstrmActlXpryDt    float64\n",
      "StrkPric               float64\n",
      "OptnTp                 float64\n",
      "FinInstrmNm             object\n",
      "OpnPric                float64\n",
      "HghPric                float64\n",
      "LwPric                 float64\n",
      "ClsPric                float64\n",
      "LastPric               float64\n",
      "PrvsClsgPric           float64\n",
      "UndrlygPric            float64\n",
      "SttlmPric              float64\n",
      "OpnIntrst              float64\n",
      "ChngInOpnIntrst        float64\n",
      "TtlTradgVol              int64\n",
      "TtlTrfVal              float64\n",
      "TtlNbOfTxsExctd          int64\n",
      "SsnId                   object\n",
      "NewBrdLotQty             int64\n",
      "Rmks                   float64\n",
      "Rsvd01                 float64\n",
      "Rsvd02                 float64\n",
      "Rsvd03                 float64\n",
      "Rsvd04                 float64\n",
      "Unnamed: 34            float64\n",
      "Rsvd1                  float64\n",
      "Rsvd2                  float64\n",
      "Rsvd3                  float64\n",
      "Rsvd4                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "directory = os.getcwd()\n",
    "print(directory)\n",
    "NSE_data = pd.read_parquet(f\"{directory}/NSE-Data-2024.parquet\")\n",
    "print(NSE_data.size)\n",
    "print(NSE_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5789c-2e90-404a-9dfc-f6ec128f246d",
   "metadata": {},
   "source": [
    "1) Calculated the volatity each day using ((high price - low price)/low price) * 100\n",
    "2) The take the average value for each stock's average volatility\n",
    "3) Perform (1) and (2) using python and using Apache spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1678fe31-00e8-403a-9a90-908775fb9031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0.065623 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import time\n",
    "\n",
    "def calculateAvgVolatility(NSE_data):\n",
    "    NSE_data[\"daily_volatility\"] = ((NSE_data[\"HghPric\"] - NSE_data[\"LwPric\"])/ NSE_data[\"LwPric\"])*100\n",
    "    NSE_average_volatility = NSE_data.groupby(\"FinInstrmNm\")[\"daily_volatility\"].mean()\n",
    "    volatility_df = NSE_average_volatility.reset_index()\n",
    "    volatility_df.columns = [\"StockName\", \"AverageVolatility\"]\n",
    "    return(volatility_df)\n",
    "    \n",
    "def time_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Execution Time: {end_time - start_time:.6f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@time_decorator\n",
    "def test_function(data):\n",
    "    return calculateAvgVolatility(data)\n",
    "\n",
    "# Call the decorated function\n",
    "python_result = test_function(NSE_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3311e8-271d-45c4-a9a5-1c65e2a1238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "def calculateAvgVolatilityWithSpark(NSE_data, spark):\n",
    "    NSE_data_pyspark_df = spark.createDataFrame(NSE_data)\n",
    "    \n",
    "    # Calculate daily volatility as a new column\n",
    "    df_with_volatility = NSE_data_pyspark_df.withColumn(\n",
    "        \"daily_volatility\",\n",
    "        ((col(\"HghPric\") - col(\"LwPric\")) / col(\"LwPric\") * 100)\n",
    "    )\n",
    "    \n",
    "    # Calculate the average daily volatility per stock\n",
    "    average_volatility_df = df_with_volatility.groupBy(\"FinInstrmNm\").agg(\n",
    "        avg(\"daily_volatility\").alias(\"average_volatility\")\n",
    "    )\n",
    "    return(average_volatility_df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda9ca69-ea4f-4033-bfd9-61e84013d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 37.491835 seconds\n"
     ]
    }
   ],
   "source": [
    "@time_decorator\n",
    "def test_function(data,spark):\n",
    "    return calculateAvgVolatilityWithSpark(data,spark)\n",
    "\n",
    "# Call the decorated function\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BasicPySparkExample\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.port\", \"6066\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "result = test_function(NSE_data,spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a57762d2-3f7b-43a0-a4c1-d5dc8664bcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         FinInstrmNm  average_volatility\n",
      "89  APOLLO TYRES LTD            2.932801\n",
      "            StockName  AverageVolatility\n",
      "263  APOLLO TYRES LTD           2.932801\n"
     ]
    }
   ],
   "source": [
    "print(result[result[\"FinInstrmNm\"] == \"APOLLO TYRES LTD\"])\n",
    "print(python_result[python_result[\"StockName\"] == \"APOLLO TYRES LTD\"])\n",
    "#result[\"FinInstrmNm\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c77b0054-a082-4292-916c-aa3ee51d9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered = NSE_data[NSE_data[\"FinInstrmNm\"] == \"APOLLO TYRES LTD\"]\n",
    "Filtered.to_csv(\"stock_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e758d9dc-97a3-4b94-bb41-d507841f52e9",
   "metadata": {},
   "source": [
    "For each stock perform below \n",
    "- Calculate monthly average price volatility - using HghPric and\tLwPric \n",
    "- Calculate monthly average stock volumn using column - TtlTradgVol\n",
    "- Aggregate the data based on sector and store the results\n",
    "- For each sector calcualte which stock has highest volatility and for each month\n",
    "- then across all secotr find which stock has highest volatility and for each month"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
